{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBUST APPROACH: Supervised learning example: regression.\n",
    "\n",
    "## ☀️ Prediction of photovoltaic generation for self-consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Predict the next day's PV generation of a household, in order to intelligently manage its consumption. \n",
    "* We will use historical data of the **target variable** we want to predict (historical PV generation data) and other features that can help to predict the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ml.png\" alt=\"Data center diagram\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Import libraries and data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import libraries\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# We load the input data set\n",
    "dataset = pd.read_csv('data/regression_PVforecasting.csv', delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data analysis: Understanding the data**\n",
    "\n",
    "It is necessary to visualize and understand the data we are going to work with, as well as to know its characteristics. \n",
    "\n",
    "1. How many rows we have? How many attributes are there in the data?  \n",
    "2. What are these attributes?\n",
    "3. Is there any missing data?\n",
    "4. Statistical summary of the input data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many attributes are there in the data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do they mean?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 5 rows\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**.dtypes** methods is essential for data cleaning and preprocessing\n",
    "\n",
    "* ``int64`` or ``float64`` → numeric → can be used for math, stats, ML models\n",
    "\n",
    "* ``object`` → usually text or mixed data → needs preprocessing\n",
    "\n",
    "* ``datetime64`` → time-aware operations (resampling, trends, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert localhour in datetime\n",
    "dataset['localhour'] = pd.to_datetime(dataset['localhour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['localhour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data types again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Is any data missing?** A check is made to see if any data is missing, and if so, the count of empty cells in each attribute is performed. In this case, no data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Statistical Summary of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data.\n",
    "\n",
    "A visual way to understand the input data. \n",
    "\n",
    "1. Boxplots and Density plots\n",
    "2. Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots\n",
    "\n",
    "The boxplot allows us to identify outliers and compare distributions. In addition, we know how 50% of the values are distributed (inside the box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos_boxplot = dataset.plot(kind='box', subplots=True, layout=(3, 3), figsize=(15, 10), sharex=False,\n",
    "                                 sharey=False, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add density plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation matrix** \n",
    "\n",
    "\n",
    "Why the correlation matrix is useful in Machine learning:\n",
    "\n",
    "* **Feature selection:** It shows how strongly each input feature is correlated with the target variable.\n",
    "    * Features with very low correlation (=0) might add little predictive value.\n",
    "    * Highly correlated features with the target are often more relevant.\n",
    "* **Detect multicollinearity**: It helps identify features that are strongly correlated with each other.\n",
    "    * Using highly correlated predictors can make models (especially linear ones) unstable or redundant.\n",
    "* **Improve interpretability and model performance**. By removing or combining correlated features, you can make the model simpler, faster, and less prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seaborn visualization library\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculation of correlation coefficients\n",
    "#Pearson for linear correlation\n",
    "#Spearman for montonous correlation\n",
    "#https://duchesnay.github.io/pystatsml/auto_gallery/ml_resampling.html\n",
    "    \n",
    "corr = dataset.corr(method='pearson') \n",
    "# Remove repeated values\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "  \n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "#Generar Heat Map,\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "    # xticks\n",
    "#plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    # yticks\n",
    "#plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    # plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "We select/add/remove features in an iterative procress of training and testing the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Add ``time`` and ``month`` columns through the datetime column. \n",
    "The data is scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month and time columns\n",
    "dataset['month'] = pd.DatetimeIndex(dataset['localhour']).month\n",
    "dataset['hour'] = pd.DatetimeIndex(dataset['localhour']).hour\n",
    "dataset.drop(['localhour'], axis=1, inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into **attributes**: X (features) and **tags**: y (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features X ; Target y \n",
    "X = dataset.drop(['pvgen'], axis=1) \n",
    "y = dataset['pvgen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data\n",
    "\n",
    "First, let's check if there is missing data we should handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in y:\")\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data separation: Split the data in train-validation-test.\n",
    "\n",
    "The data are divided into training data ``X_train``, ``y_train``, validation data ``X_val``, ``y_val`` and test data ``X_test``, ``y_test``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2  # percentage of the input data that I will use to validate the model\n",
    "  \n",
    "# I divide the data into training, validation and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where is the missing data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X train, X val and X test:\")\n",
    "print(X_train.isnull().sum())\n",
    "print(X_val.isnull().sum())\n",
    "print(X_test.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data\n",
    "\n",
    "####  **For specific imputation methods, you can not impute before splitting!**\n",
    "\n",
    "If you impute before splitting into train/test, you will be using information from the test set (its statistics like mean/median) to fill missing values in the training data. \n",
    "\n",
    "That's called **data leakage** — it contaminates your training process and gives over-optimistic results.\n",
    "\n",
    "* Correct approach when using the entire dataset for data imputation:\n",
    "\n",
    "    * Split the data → training & testing.\n",
    "    * Fit the imputer only on the training data.\n",
    "    * Apply (transform) it to both the training and test sets.\n",
    "    \n",
    "    \n",
    "PS: If you use some other imputation methods, like backfll/interpolation/etc. then there is no data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply mean imputation on training, validation, and test sets\n",
    "X_train_imputed = X_train.fillna(X_train.mean())\n",
    "X_val_imputed   = X_val.fillna(X_train.mean())   # use training mean to avoid data leakage\n",
    "X_test_imputed  = X_test.fillna(X_train.mean())  # same: use training mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values after mean imputation:\\n\")\n",
    "\n",
    "print(\"X_train missing values per column:\")\n",
    "print(X_train_imputed.isnull().sum(), \"\\n\")\n",
    "\n",
    "print(\"X_val missing values per column:\")\n",
    "print(X_val_imputed.isnull().sum(), \"\\n\")\n",
    "\n",
    "print(\"X_test missing values per column:\")\n",
    "print(X_test_imputed.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data\n",
    "\n",
    "Different scalers handle data in different ways, depending on the algorithm and the data’s nature\n",
    "\n",
    "* **StandardScaler (Z-score normalization)**   ``from sklearn.preprocessing import StandardScaler ``\n",
    "    * Centers data around mean = 0 and standard deviation = 1.\n",
    "    * Keeps outliers but rescales the overall distribution.\n",
    "* **MinMaxScaler (Normalization to a range)**  ``from sklearn.preprocessing import MinMaxScaler``\n",
    "    * Rescales features to a fixed range (by default [0, 1]).\n",
    "    * Preserves shape of original distribution, but sensitive to outliers.\n",
    "* **RobustScaler (less sensitive to outliers)**  ``from sklearn.preprocessing import RobustScaler``\n",
    "    * Uses median and interquartile range (IQR) instead of mean and std.\n",
    "    * Great for datasets with outliers or skewed distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this scenario,  **data is scaled** using the ``MinMaxScaler()`` method, which scales and translates each attribute individually such that it is within the range [0, 1]. This needs to be done when the scales of the attributes are different (e.g. radiation [0, 650], wind speed [2, 15])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale attributes/features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_imputed),\n",
    "    columns=X_train_imputed.columns)\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the scaler to X_val and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform validation and test data using the same scaler\n",
    "\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val_imputed),\n",
    "    columns=X_val_imputed.columns\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_imputed),\n",
    "    columns=X_test_imputed.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model building and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The selected evaluation metrics are **RMSE and R2**.\n",
    "* Check all the available evaluation metrics in Scikit Learn https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "![available evaluation metrics](./Figures/regression_evaluationmetric.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# number of folds for cross-validation\n",
    "\n",
    "num_folds =3  # To make the code run faster in class, I have reduced the number of folds. But this number should be somewhat higher. \n",
    "\n",
    "error_metrics = {'r2'}\n",
    "models = {('MLP', MLPRegressor()),('RFR', RandomForestRegressor()), ('KNN', KNeighborsRegressor())}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the models is trained, the results are saved and compared visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "\n",
    "# Cross-validation training\n",
    "for scoring in error_metrics:\n",
    "    results = [] # store metrics results\n",
    "    msg = []  # print summary of result\n",
    "    names = []  # store name of the models\n",
    "    print('Evaluation metric: ', scoring)\n",
    "    for name, model in models:\n",
    "        print('Model ', name)\n",
    "        cross_validation = TimeSeriesSplit(n_splits=num_folds)\n",
    "        cv_results = cross_val_score(model, X_train_scaled, y_train, cv=cross_validation, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        resume = (name, cv_results.mean(), cv_results.std())\n",
    "        msg.append(resume)\n",
    "    print(msg)\n",
    "\n",
    "    # Compare results between algorithms\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Compare metric result for algorithms: %s' %scoring)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('Candidate models')\n",
    "    ax.set_ylabel('%s' %scoring)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()\n",
    "\n",
    "    results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Best models hyperparameters adjustment.\n",
    "\n",
    "Steps to perform the hyperadjustment of the parameters:\n",
    "* Specify the model to be adjusted\n",
    "* Specify a metric to optimize\n",
    "* Define the search parameter ranges: *params*\n",
    "* Assign a validation method: *KFold*\n",
    "* Find the Hyperparameters with the validation data: *X_val*\n",
    "\n",
    "\n",
    "**To make the code run faster in class, we have only used two possible values for the ``n_estimators`` hyperparameter. In reality, the more hyperparameters you test, the better. And the more values per hyperparameter, too. But this can be computationally very intensive. Find a good trade-off.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = RandomForestRegressor()\n",
    "scoring='r2'\n",
    "\n",
    "\n",
    "params = {\n",
    "    # Number of trees in random forest\n",
    "    'n_estimators': [100, 200, 500, 1000],  # default=100\n",
    "     # Maximum number of levels in tree\n",
    "    'max_depth': [2, None],  #deafult = None\n",
    "     # Method of selecting samples for training each tree\n",
    "}\n",
    "\n",
    "\n",
    "# Search for the best combination of hyperparameters\n",
    "cross_validation = TimeSeriesSplit(n_splits=5)\n",
    "my_cv = cross_validation.split(X_val_scaled)\n",
    "\n",
    "# Grid search with verbose output\n",
    "gsearch = GridSearchCV(\n",
    "    estimator=modelo,\n",
    "    param_grid=params,\n",
    "    scoring=scoring,\n",
    "    cv=my_cv,\n",
    "    verbose=2,    #  enables detailed progress logs\n",
    "    n_jobs=-1     #  uses all available CPU cores\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting grid search...\\n\")\n",
    "gsearch.fit(X_val_scaled, y_val)\n",
    "print(\"\\nGrid search completed.\")\n",
    "\n",
    "\n",
    "# Print best Result\n",
    "print(\"Best result: %f using the following hyperparameters %s\" % (gsearch.best_score_, gsearch.best_params_))\n",
    "means = gsearch.cv_results_['mean_test_score']\n",
    "stds = gsearch.cv_results_['std_test_score']\n",
    "params = gsearch.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final evaluation of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, PV generation predictions are made.\n",
    "\n",
    "Evaluation metrics:\n",
    "  * RMSE\n",
    "  * R2\n",
    "\n",
    "    \n",
    "The ``fit()`` model is trained with the optimal hyperparameters found in the previous section and then the predictions are made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = RandomForestRegressor(n_estimators=100, max_depth= None) ## train again with the winner model from the Grid Search\n",
    "final_model.fit(X_train_scaled,y_train)  # Model training \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the model is trained, congrats! Let's make predictions with that model, and see if the outcomes are good or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_predict = final_model.predict(X_test_scaled)  # prediction calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Error RMSE de test  \n",
    "print('RMSE: ', math.sqrt(mean_squared_error(y_test, y_predict)))\n",
    "print('R2: ', r2_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 Evaluation metrics: \n",
    "* Train 0.91\n",
    "* Val 0.95\n",
    "* Test 0.88\n",
    "\n",
    "* --> There is no overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph results obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_predict vs y_test\n",
    "\n",
    "x = range(len(y_predict))\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.xlabel('Time', size=15)\n",
    "plt.ylabel('Energy produced (kWh)', size=15)\n",
    "plt.plot(x, y_predict, alpha=0.4, color='blue', label='PV predict')\n",
    "plt.plot(x, y_test, alpha=0.4, color='red',  label='PV real')\n",
    "plt.title('Prediction vs Real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to Zoom in!\n",
    "\n",
    "If necessary, install the Plotly library ``!pip install plotly``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go  # Importamos la librería de plotly\n",
    "\n",
    "init = list(range(len(y_predict)))\n",
    "y_predict_plot = pd.DataFrame(data=y_predict, index=init, columns=['predict'])\n",
    "\n",
    "# Reindex y_test so it starts from 0, ensuring y_predict and y_test share the same index for plotting\n",
    "y_test_plot = pd.DataFrame(data=y_test.values, index=init, columns=['test'])\n",
    "\n",
    "\n",
    "# We create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=init, y=y_predict_plot['predict'][init],\n",
    "                    mode='lines',\n",
    "                    name='PV prediction'))\n",
    "fig.add_trace(go.Scatter(x=init, y=y_test_plot['test'][init],\n",
    "                     mode='lines', name='PV real'))\n",
    "\n",
    "\n",
    "# We edit figure\n",
    "fig.update_layout(autosize=False,\n",
    "                  width=1000,\n",
    "                    height=500,\n",
    "                    title='Prediction vs Real',\n",
    "                   xaxis_title='Periods',\n",
    "                   yaxis_title='Energy (kWh)')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Most important features/attributes \n",
    "\n",
    "Which features carry the most weight in this example? \n",
    "\n",
    "* Not to be confused with correlation matrix, which is not the same thing. For more info: tree.feaure_importances https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html \n",
    "\n",
    "* This does not work with all ML regression models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We print the feature ranking\n",
    "importances = gsearch.best_estimator_.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in gsearch.best_estimator_.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feat = X.columns\n",
    "feat_or=[]\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]])+feat[indices[f]])\n",
    "    feat_or.append(feat[indices[f]])\n",
    "\n",
    "# We plot the weight of the features that matter most for RandomForest()\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), feat_or, rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
