{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLER APPROACH: Supervised learning example: regression.\n",
    "\n",
    "## ☀️ Prediction of photovoltaic generation for self-consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Predict the next day's PV generation of a household, in order to intelligently manage its consumption. \n",
    "* We will use historical data of the **target variable** we want to predict (historical PV generation data) and other features that can help to predict the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/ml.png\" alt=\"Data center diagram\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Import libraries and data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import libraries\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# We load the input data set\n",
    "dataset = pd.read_csv('data/regression_PVforecasting.csv', delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data analysis: Understanding the data**\n",
    "\n",
    "It is necessary to visualize and understand the data we are going to work with, as well as to know its characteristics. \n",
    "\n",
    "1. How many rows we have? How many attributes are there in the data?  \n",
    "2. What are these attributes?\n",
    "3. Is there any missing data?\n",
    "4. Statistical summary of the input data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many attributes are there in the data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do they mean?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 5 rows\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**.dtypes** methods is essential for data cleaning and preprocessing\n",
    "\n",
    "* ``int64`` or ``float64`` → numeric → can be used for math, stats, ML models\n",
    "\n",
    "* ``object`` → usually text or mixed data → needs preprocessing\n",
    "\n",
    "* ``datetime64`` → time-aware operations (resampling, trends, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert localhour in datetime\n",
    "dataset['localhour'] = pd.to_datetime(dataset['localhour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['localhour']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data types again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Is any data missing?** A check is made to see if any data is missing, and if so, the count of empty cells in each attribute is performed. In this case, no data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Statistical Summary of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data.\n",
    "\n",
    "A visual way to understand the input data. \n",
    "\n",
    "1. Boxplots and Density plots\n",
    "2. Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots\n",
    "\n",
    "The boxplot allows us to identify outliers and compare distributions. In addition, we know how 50% of the values are distributed (inside the box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos_boxplot = dataset.plot(kind='box', subplots=True, layout=(3, 3), figsize=(15, 10), sharex=False,\n",
    "                                 sharey=False, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation matrix** \n",
    "\n",
    "Why the correlation matrix is useful in Machine learning:\n",
    "\n",
    "* **Feature selection:** It shows how strongly each input feature is correlated with the target variable.\n",
    "    * Features with very low correlation (=0) might add little predictive value.\n",
    "    * Highly correlated features with the target are often more relevant.\n",
    "* **Detect multicollinearity**: It helps identify features that are strongly correlated with each other.\n",
    "    * Using highly correlated predictors can make models (especially linear ones) unstable or redundant.\n",
    "* **Improve interpretability and model performance**. By removing or combining correlated features, you can make the model simpler, faster, and less prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seaborn visualization library\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculation of correlation coefficients\n",
    "#Pearson for linear correlation\n",
    "#Spearman for montonous correlation\n",
    "#https://duchesnay.github.io/pystatsml/auto_gallery/ml_resampling.html\n",
    "    \n",
    "corr = dataset.corr(method='pearson') \n",
    "# Remove repeated values\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "  \n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "#Generar Heat Map,\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "    # xticks\n",
    "#plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    # yticks\n",
    "#plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    # plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "We select/add/remove features in an iterative procress of training and testing the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Add ``time`` and ``month`` columns through the datetime column. \n",
    "The data is scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month columns\n",
    "dataset['month'] = pd.DatetimeIndex(dataset['localhour']).month\n",
    "dataset['hour'] = pd.DatetimeIndex(dataset['localhour']).hour\n",
    "dataset.drop(['localhour'], axis=1, inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into **attributes**: X (features) and **tags**: y (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features X ; Target y \n",
    "X = dataset.drop(['pvgen'], axis=1) \n",
    "y = dataset['pvgen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data\n",
    "\n",
    "First, let's check if there is missing data we should handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in y:\")\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interpolate missing values ---\n",
    "# For the feature matrix X\n",
    "X_interpolated = X.interpolate(method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X:\")\n",
    "print(X_interpolated.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data separation: Split the data in train-validation-test.\n",
    "\n",
    "The data are divided into training data ``X_train``, ``y_train``, validation data ``X_val``, ``y_val`` and test data ``X_test``, ``y_test``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2  # percentage of the input data that I will use to validate the model\n",
    "  \n",
    "# I divide the data into training, validation and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_interpolated, y, test_size=test_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data\n",
    "\n",
    "Different scalers handle data in different ways, depending on the algorithm and the data’s nature\n",
    "\n",
    "* **StandardScaler (Z-score normalization)**   ``from sklearn.preprocessing import StandardScaler ``\n",
    "    * Centers data around mean = 0 and standard deviation = 1.\n",
    "    * Keeps outliers but rescales the overall distribution.\n",
    "* **MinMaxScaler (Normalization to a range)**  ``from sklearn.preprocessing import MinMaxScaler``\n",
    "    * Rescales features to a fixed range (by default [0, 1]).\n",
    "    * Preserves shape of original distribution, but sensitive to outliers.\n",
    "* **RobustScaler (less sensitive to outliers)**  ``from sklearn.preprocessing import RobustScaler``\n",
    "    * Uses median and interquartile range (IQR) instead of mean and std.\n",
    "    * Great for datasets with outliers or skewed distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this scenario,  **data is scaled** using the ``MinMaxScaler()`` method, which scales and translates each attribute individually such that it is within the range [0, 1]. This needs to be done when the scales of the attributes are different (e.g. radiation [0, 650], wind speed [2, 15])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale attributes/features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train),columns=X_train.columns)\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the scaler to X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform  test data using the same scaler\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 & 6. Model building and final test evaluation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The selected evaluation metrics are **RMSE**.\n",
    "* Check all the available evaluation metrics in Scikit Learn https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "![available evaluation metrics](./Figures/regression_evaluationmetric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important methods:\n",
    "\n",
    "``fit()`` – Function used for training the ML model\n",
    "* Purpose: Teaches the model the relationship between the input features (X_train) and the target variable (y_train).\n",
    "* What it does: Finds model parameters (e.g., coefficients, tree splits, neural network weights, etc. depending on the ML model selected) that minimize prediction error.\n",
    "* Output: A trained model ready to make predictions.\n",
    "\n",
    "``predict()`` – Method for making predictions\n",
    "\n",
    "* Purpose: Uses the trained model to make predictions on new or unseen data.\n",
    "* What it does: Applies the learned parameters from training to compute output values (y_pred).\n",
    "* Output: Predicted labels or values (regression outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "# --- Define and train each model ---\n",
    "# RANDOM FOREST\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "y_rf_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_knn_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# NEURAL NETWORK MLP\n",
    "mlp = MLPRegressor()\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_mlp_pred = mlp.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ``evaluate_model()`` function is a helper function used to assess the performance of regression models.\n",
    "* This produces a clear comparison of the Random Forest, K-Nearest Neighbors, and Neural Network (MLP) models in terms of their predictive accuracy and overall performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_true, y_pred):.4f}\")\n",
    "    print(f\"MSE: {mean_squared_error(y_true, y_pred):.4f}\")\n",
    "    print(f\"R²:  {r2_score(y_true, y_pred):.4f}\\n\")\n",
    "\n",
    "evaluate_model(\"Random Forest\", y_test, y_rf_pred)\n",
    "evaluate_model(\"KNN\", y_test, y_knn_pred)\n",
    "evaluate_model(\"MLP\", y_test, y_mlp_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the models is trained, the results are saved and compared visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the model is trained, congrats! Let's make predictions with that model, and see if the outcomes are good or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph results obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_predict vs y_test\n",
    "\n",
    "x = range(len(y_rf_pred))\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.xlabel('Time', size=15)\n",
    "plt.ylabel('Energy produced (kWh)', size=15)\n",
    "\n",
    "plt.plot(x, y_rf_pred, alpha=0.4, color='blue', label='RF PV predict')\n",
    "plt.plot(x, y_knn_pred, alpha=0.4, color='green', label='KNN PV predict')\n",
    "plt.plot(x, y_mlp_pred, alpha=0.4, color='black', label='MLP PV predict')\n",
    "\n",
    "plt.plot(x, y_test, alpha=0.4, color='red',  label='PV real')\n",
    "plt.title('Prediction vs Real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to Zoom in!\n",
    "\n",
    "If necessary, install the Plotly library ``!pip install plotly``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go  # Importamos la librería de plotly\n",
    "\n",
    "init = list(range(len(y_rf_pred)))\n",
    "\n",
    "y_predict_rf = pd.DataFrame(data=y_rf_pred, index=init, columns=['RF PV predict'])\n",
    "y_predict_knn = pd.DataFrame(data=y_knn_pred, index=init, columns=['KNN PV predict'])\n",
    "y_predict_milp = pd.DataFrame(data=y_mlp_pred, index=init, columns=['MLP PV predict'])\n",
    "\n",
    "# Reindex y_test so it starts from 0, ensuring y_predict and y_test share the same index for plotting\n",
    "y_test_plot = pd.DataFrame(data=y_test.values, index=init, columns=['test'])\n",
    "\n",
    "# We create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=init, y=y_predict_rf['RF PV predict'][init],\n",
    "                    mode='lines',\n",
    "                    name='RF PV prediction'))\n",
    "fig.add_trace(go.Scatter(x=init, y=y_predict_knn['KNN PV predict'][init],\n",
    "                    mode='lines',\n",
    "                    name='KNN PV prediction'))\n",
    "fig.add_trace(go.Scatter(x=init, y=y_predict_milp['MLP PV predict'][init],\n",
    "                    mode='lines',\n",
    "                    name='MILP PV prediction'))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=init,\n",
    "    y=y_test_plot['test'][init],\n",
    "    mode='lines',\n",
    "    name='PV real',\n",
    "    line=dict(color='black', width=0.7) \n",
    "))\n",
    "\n",
    "\n",
    "# We edit figure\n",
    "fig.update_layout(autosize=False,\n",
    "                  width=1000,\n",
    "                    height=500,\n",
    "                    title='Prediction vs Real',\n",
    "                   xaxis_title='Periods',\n",
    "                   yaxis_title='Energy (kWh)')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
